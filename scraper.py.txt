# scraper.py
import os
import time
import tempfile
import requests
from bs4 import BeautifulSoup
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client

# ---- load local .env (для локальной разработки) ----
load_dotenv()

SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY')
SUPABASE_BUCKET = os.environ.get('SUPABASE_BUCKET', 'product-images')
TARGET_BASE_URL = os.environ.get('TARGET_BASE_URL')  # страница каталога

if not SUPABASE_URL or not SUPABASE_KEY or not TARGET_BASE_URL:
    raise RuntimeError("Проверьте .env: SUPABASE_URL, SUPABASE_KEY и TARGET_BASE_URL должны быть заполнены")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

HEADERS = {"User-Agent": "MyShopBot/1.0 (+your-email@example.com)"}

# --- НАСТРОЙТЕ СЕЛЕКТОРЫ ПОД ЦЕЛЕВОЙ САЙТ ---
CONTAINER_SELECTOR = ".product-card"   # замените на селектор карточки товара
NAME_SELECTOR = ".title"               # селектор названия внутри карточки
IMG_SELECTOR = "img"                   # селектор для <img> внутри карточки
AVAIL_SELECTOR = ".stock"              # селектор наличия (если есть)
# ------------------------------------------------

def download_image_to_temp(url):
    resp = requests.get(url, headers=HEADERS, timeout=20)
    resp.raise_for_status()
    fd, path = tempfile.mkstemp(suffix=".jpg")
    with open(path, "wb") as f:
        f.write(resp.content)
    return path

def upload_file_to_supabase(local_path, bucket_path):
    with open(local_path, "rb") as f:
        try:
            supabase.storage.from_(SUPABASE_BUCKET).upload(bucket_path, f)
        except Exception:
            # пробуем альтернативный вызов, если пакетная версия другая
            supabase.storage.from_(SUPABASE_BUCKET).upload(bucket_path, local_path)
    public = supabase.storage.from_(SUPABASE_BUCKET).get_public_url(bucket_path)
    public_url = public.get('publicURL') if isinstance(public, dict) else public
    return public_url

def upsert_product_to_db(product_dict):
    res = supabase.table('products').upsert(product_dict).execute()
    return res

def parse_catalog_page():
    print("Запрос к", TARGET_BASE_URL)
    r = requests.get(TARGET_BASE_URL, headers=HEADERS, timeout=30)
    r.raise_for_status()
    soup = BeautifulSoup(r.text, "html.parser")
    items = soup.select(CONTAINER_SELECTOR)
    print(f"Найдено карточек: {len(items)}")
    now = datetime.utcnow().isoformat()

    for idx, it in enumerate(items, start=1):
        try:
            name_el = it.select_one(NAME_SELECTOR)
            name = name_el.get_text(strip=True) if name_el else "no-name"
            external_id = it.get("data-id") or it.get("data-product-id") or name[:60].replace(" ", "_")
            avail_el = it.select_one(AVAIL_SELECTOR)
            availability = avail_el.get_text(strip=True) if avail_el else "unknown"
            img_el = it.select_one(IMG_SELECTOR)
            img_src = img_el.get("src") if img_el else None
            if not img_src:
                print(f"[{idx}] Пропускаю: нет картинки для {name}")
                continue
            img_url = requests.compat.urljoin(TARGET_BASE_URL, img_src)

            print(f"[{idx}] {external_id} — {name} — {availability} — {img_url}")

            tmp_path = download_image_to_temp(img_url)
            bucket_path = f"products/{external_id}.jpg"
            public_url = upload_file_to_supabase(tmp_path, bucket_path)

            prod = {
                "external_id": external_id,
                "name": name,
                "image_url": public_url,
                "external_image_url": img_url,
                "availability": availability,
                "last_seen": now,
                "last_scraped": now
            }
            upsert_product_to_db(prod)

            try:
                os.remove(tmp_path)
            except:
                pass

            time.sleep(1.5)
        except Exception as e:
            print("Ошибка при обработке карточки:", e)

if __name__ == "__main__":
    parse_catalog_page()
    print("Готово.")
